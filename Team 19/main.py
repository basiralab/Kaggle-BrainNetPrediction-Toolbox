"""
Target Problem:
---------------
* Predict the evolution of brain connectivity over time.

Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* Inverse Sigmoid Function -> SelectKBest -> Support Vector Regression

Input to Proposed Solution:
---------------------------
* Directories of training data and labels(measurements at two different timepoints), and testing data in csv file format.
* These data should be stored in n x m pattern in csv file format.

  Typical Example:
  ----------------
  n x m samples in train_t0 csv file (n number of samples, m number of features)
  n x m samples in train_t1 csv file (n number of samples, m number of features)
  k x m samples in test_t0 csv file (k number of samples, m number of features)

* These data set files are ready by load_data() function.

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "submission.csv" file.

Code Owner:
-----------
* Copyright © Team 19. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Fall 2021. All rights reserved.
"""


import numpy as np 
import pandas as pd 
import statsmodels.api as sm
from sklearn.feature_selection import SelectKBest, mutual_info_regression
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR
from scipy.stats import pearsonr

np.random.seed(1)

class MultioutputLearner:
    def __init__(self,X,Y):
        self.X = np.array(X).copy() #X is the t0 values of dataset
        self.Y = np.array(Y).copy() #Y is the t1 values of dataset, they are the future values (labels)
        self.listofLR = [] #array for saving learning models, includes 595 learned model, one for every dimension of y(label)
        self.listofFS = [] #array for saving dimensionality reduction techniques, includes 595 learned model, one for every dimension of y(label)
    
    #function for training the dataset
    #It takes the multioutput class and desired number of features as input
    def train(self,n_features):
        for i in range(self.Y.shape[1]): #iterating through 595 brain connectivity t1 values 
            temp_y = preprocessing(self.Y[:,i]).copy() #preprocessing the t1 value
            FSmodel = SelectKBest(mutual_info_regression, k=n_features).fit(self.X, temp_y) 
            new_X = FSmodel.transform(self.X) #selecting 'n_features' features in order to decrease the dimensionality
            temp_model = SVR().fit(new_X,temp_y) #creating the model
            self.listofLR.append(temp_model) #adding learned model to list
            self.listofFS.append(FSmodel) #adding dimensionality reduction technique to list

    #function for testing the dataset
    #It takes the multioutput class and testset of the dataset as input
    #returns the predicted t1 values
    def test(self,testset):
        testoutput = [] #array for saving predicted t1 values
        testset = np.array(testset).copy() #changing the testset's type to numpy array
        for i in range(len(self.listofLR)): #traversing through learned models
            new_test = self.listofFS[i].transform(testset) #applying dimensionality reduction to testset
            prediction = self.listofLR[i].predict(new_test).copy() #predicting the t1 values
            testoutput.append(prediction) #adding predictions to array
        # returning the transpose of predicted values, since appending prediction adds predictions one after other
        # but we should add predictions side by sides, hence transpose will provide us the desired format
        post_y = np.array(testoutput)
        for i in range(post_y.shape[0]):
            for j in range(post_y.shape[1]):
                post_y[i][j] = 1/(1 + np.exp(-post_y[i][j])) #sigmoid function
        return np.transpose(post_y)     

def load_data(train_file, test_file):

    """
    The method reads train and test data from their dataset files.
    Then, it splits train data into features and labels.
    Parameters
    ----------
    train_file: directory of the file in which train data set is located
    test_file: directory of the file in which test data set is located
    """

    x_tra = pd.read_csv(train_file[0])
    y_tra = pd.read_csv(train_file[1])
    x_tst = pd.read_csv(test_file)
    return x_tra, y_tra, x_tst


def preprocessing(y_tra):

    """
    This method performs inverse sigmoid function on the data.
    ----------
    y_tra: labels of training data
    """
    
    y = np.array(y_tra).copy()
    epsilon = 1e-100
    for i in range(y.shape[0]):
        if(y[i] > 1):
            y[i] = 1
        if(y[i] < 0):
            y[i] = 0
        if((1-y[i]) < epsilon): #to avoid overflow in model
            y[i] = 1e+10
        elif(y[i] < epsilon): #to avoid overflow in model
            y[i] = -1e+10
        else:
            temp = y[i]
            y[i] = np.log(temp/(1-temp)) #inverse sigmoid function
    return y

def train_model(x_tra, y_tra):

    """
    The method creates a learning model and trains it by using training data.
    Parameters
    ----------
    x_tra: features of training data
    y_tra: labels of training data
    """

    #adding train datasets of t0 and t1 values side by side
    all_train = np.append( np.delete(np.array(x_tra).copy(), [0], axis=1), np.delete(np.array(y_tra).copy(), [0], axis=1) ,axis=1)
    #creating k folds from train datasets
    kf = KFold(n_splits=5, shuffle = True, random_state = 1)
    models = []
    #k-fold cross validation
    for train_index,test_index in kf.split(all_train):
        train_fold = np.array((all_train[train_index])) #getting train fold
        fold_t0 = train_fold[:,0:int(train_fold.shape[1]/2)] #getting features of train fold
        fold_t1 = train_fold[:,int(train_fold.shape[1]/2):train_fold.shape[1]] #getting labels of train fold
        model = MultioutputLearner(fold_t0,fold_t1) #creating the multioutputlearner class from train fold
        model.train(130) #training the fold
        models.append(model)
    return models

def predict(x_tst, model):

    """
    The method predicts labels for testing data samples by using trained learning model.
    Parameters
    ----------
    x_tst: features of testing data
    model: trained learning model
    """

    test = np.delete(np.array(x_tst).copy(), [0], axis=1)
    #creating an empty test_result array in order to save predictions
    testresult = np.zeros((test.shape[0],int(test.shape[1])))

    for model_ in model:
        prediction = model_.test(test)
        testresult += prediction
    return testresult/len(model)

def cross_validation(x_tra, y_tra):

    """
    The method performs 5-fold cross-validation on training data and returns the error rates.
    Parameters
    ----------
    x_tra: features of training data
    y_tra: labels of training data
    """

    # adding train datasets of t0 and t1 values side by side
    all_train = np.append( np.delete(np.array(x_tra).copy(), [0], axis=1), np.delete(np.array(y_tra).copy(), [0], axis=1) ,axis=1)
    #creating k folds from train datasets
    kf = KFold(n_splits=5, shuffle = True, random_state = 1)
    #for saving all predictions
    total_prediction = np.delete(np.array(x_tra).copy(), [0], axis=1)
    #k-fold cross validation
    for train_index,test_index in kf.split(all_train):
        #getting train fold
        train_fold = np.array((all_train[train_index]))
        fold_t0 = train_fold[:,0:int(train_fold.shape[1]/2)] #getting features of train fold
        fold_t1 = train_fold[:,int(train_fold.shape[1]/2):train_fold.shape[1]] #getting labels of train fold
        #getting test fold
        test_fold = np.array((all_train[test_index]))
        test_fold_t0 = test_fold[:,0:int(test_fold.shape[1]/2)]#getting features of test fold
        test_fold_t1 = test_fold[:,int(test_fold.shape[1]/2):test_fold.shape[1]] #getting labels of test fold
        #creating the multioutputlearner class from train fold
        model = MultioutputLearner(fold_t0,fold_t1) 
        model.train(130) #training the train fold
        prediction = model.test(test_fold_t0) #testing the test fold
        total_prediction[test_index] = prediction #saving prediction
    #calculating the mean squared error between predicted train_t1 values and real train_t1 values
    total_error = mean_squared_error(np.delete(np.array(y_tra).copy(), [0], axis=1),total_prediction)
    #calculating the pearson correlation between predicted train_t1 values and real train_t1 values
    pears_err,p = pearsonr(total_prediction.flatten(),np.delete(np.array(y_tra).copy(), [0], axis=1).flatten())
    print("Mean Squared Error: ", total_error)
    print("Pearson Correlation: ",pears_err," ~ p-value: ",p)
    return total_error #returning mean squared error

def write_output(filename, predictions):
    
    """
    Writes the outputted predictions to a file.
    Parameters
    ----------
    filename: file path for saving file
    predictions: pandas.dataframe object containing predictions
    """

    meltedDF = predictions.flatten()
    kaglexd = pd.DataFrame(meltedDF)
    kaglexd_melted = kaglexd.melt()
    kaglexd_melted.drop('variable', inplace=True, axis=1)

    kaglexd_melted = kaglexd_melted.rename(columns={'value': 'predicted'})
    kaglexd_melted.index.name = 'ID'

    kaglexd_melted.to_csv(filename)

# ********** MAIN PROGRAM ********** #


train_file, test_file = ["train_t0.csv", "train_t1.csv"], "test_t0.csv"
x_tra, y_tra, x_tst = load_data(train_file, test_file)

cross_validation(x_tra, y_tra)

model = train_model(x_tra, y_tra)
predictions = predict(x_tst, model)
write_output("submission.csv", predictions)