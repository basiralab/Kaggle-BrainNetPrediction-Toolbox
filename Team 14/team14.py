"""
Target Problem:
---------------
* To train a model to predict the brain connectivity for the next time point given the brain connectivity at current time point.
Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* Preprocessing Method (if any) -> Dimensionality Reduction method (if any) -> Learner
Input to Proposed Solution:
---------------------------
* Directories of training and testing data in csv file format
* These two types of data should be stored in n x m pattern in csv file format.
  Typical Example:
  ----------------
  n x m samples in training csv file (Explain n and m) 
  k x s samples in testing csv file (Explain k and s

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "results_team14.csv" file. (Change the name file if needed)
Code Owner:
-----------
* Copyright © Team 14. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Spring/Fall 2020. All rights reserved. 
"""


import pandas as pd
from sklearn.model_selection import KFold
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import Ridge
from scipy.stats.stats import pearsonr
import numpy as np
import random as r
#Eliminating outliers from y
from sklearn.ensemble import IsolationForest
r.seed(1)
np.random.seed(1)
SEED = 1
import warnings
warnings.filterwarnings('ignore')

def load_data(csv):

    """
    The method reads train and test data from their dataset files.
    Then, it splits train data into features and labels.
    Parameters
    ----------
    train_file: directory of the file in which train data set is located
    test_file: directory of the file in which test data set is located
    """
    return pd.read_csv(csv, index_col='ID')


def preprocessing(x, y, x_test):

     
    """
    *Eliminate features with only 0 values
    *Eliminating outliers from X using IsolationForest
    ----------
    x: input data to process
    y: target data
    x_test: test data
    """
    selector_x = VarianceThreshold(0.0000001)
    X = pd.DataFrame(selector_x.fit_transform(x))
    X_TEST = pd.DataFrame(selector_x.transform(x_test))

    isf = IsolationForest(n_jobs=-1, random_state=SEED)
    isf.fit(y)
    outliers = isf.predict(y)
    drops = (np.where(isf.predict(y) == -1))[0]

    X = X[np.where(outliers == 1, True, False)]
    y = y[np.where(outliers == 1, True, False)]

    y['new_index'] = np.arange(y.shape[0])
    y = y.set_index('new_index')
    X['new_index'] = np.arange(y.shape[0])
    X = X.set_index('new_index')

    isf = IsolationForest(n_jobs=-1, random_state=SEED)
    isf.fit(X)
    outliers = isf.predict(X)
    drops = (np.where(isf.predict(X) == -1))[0]

    X = X[np.where(outliers == 1, True, False)]
    y = y[np.where(outliers == 1, True, False)]

    y['new_index'] = np.arange(y.shape[0])
    y = y.set_index('new_index')
    X['new_index'] = np.arange(y.shape[0])
    X = X.set_index('new_index')

    return X, y, X_TEST


def train_model(X):

    """
    The method creates a learning model and trains it by using training data.
    Parameters
    ----------
    X: training data
    """
    model = Ridge(alpha=3.5, random_state=SEED, fit_intercept=True, solver='auto')
    model.fit(X, np.log1p(y))
    return model

def predict(X, ridge):

    """
    The method predicts for testing data samples by using trained learning model.
    Parameters
    ----------
    X: features of testing data
    ridge: trained  model
    """
    return np.expm1(ridge.predict(X))


def write_output(filename, predictions):

    """
    Writes predictions as desired in the submission process
    Parameters
    ----------
    filename: file path for saving file
    predictions: model outputs
    """
    submission = pd.DataFrame({
            "ID": np.arange(len(predictions.flatten())),
            "predicted": predictions.flatten()
        })
    submission.to_csv(filename+'.csv', index=False)

def cv5(X, y):

    """
    Applies 5 fold cross validation on given dataset
    Parameters
    ----------
    X: train_t0
    y: train_t1
    """
    predictions = []
    mse = []
    mae = []
    pear = []

    kf = KFold(n_splits=5, shuffle = True ,random_state=1)
    for train_index, test_index in kf.split(X):

        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]
        
        #Eliminating features with only 0 values
        selector_x = VarianceThreshold(0.0000001)
        X_train = pd.DataFrame(selector_x.fit_transform(X_train))
        X_test = pd.DataFrame(selector_x.transform(X_test))
        
        isf = IsolationForest(n_jobs=-1, random_state=SEED)
        isf.fit(y_train)
        outliers = isf.predict(y_train)
        drops = (np.where(isf.predict(y_train) == -1))[0]

        X_train = X_train[np.where(outliers == 1, True, False)]
        y_train = y_train[np.where(outliers == 1, True, False)]

        #Arrange indexes
        y_train['new_index'] = np.arange(y_train.shape[0])
        y_train = y_train.set_index('new_index')
        X_train['new_index'] = np.arange(X_train.shape[0])
        X_train = X_train.set_index('new_index')
        
        #Eliminating outliers from X
        isf = IsolationForest(n_jobs=-1, random_state=SEED)
        isf.fit(X_train)
        outliers = isf.predict(X_train)
        drops = (np.where(isf.predict(X_train) == -1))[0]
        
        X_train = X_train[np.where(outliers == 1, True, False)]
        y_train = y_train[np.where(outliers == 1, True, False)]

        #Arrange indexes
        y_train['new_index'] = np.arange(y_train.shape[0])
        y_train = y_train.set_index('new_index')
        X_train['new_index'] = np.arange(X_train.shape[0])
        X_train = X_train.set_index('new_index')
        
        ridge.fit(X_train, np.log1p(y_train))
        
        y_predicted = np.expm1(ridge.predict(X_test))
        
        mse.append(mean_squared_error(y_predicted, y_test))
        mae.append(mean_absolute_error(y_predicted, y_test))
        pear.append(pearsonr(y_predicted.flatten(), y_test.values.flatten())[0])
        predictions.append(y_predicted)

        # for i in range(y_predicted.shape[0]):
        #     print(mean_absolute_error(y_predicted[i,:], y_test.values[i,:] ))
        # print("----")
        # for i in range(y_predicted.shape[0]):
        #     print(pearsonr(y_predicted[i,:], y_test.values[i,:])[0])
        # print("----")

    print("mse: ", (mse))
    print("mae: ", (mae))
    print("pear: ",(pear))
    print("avg mse: ", np.mean(mse))
    print("avg mae: ", np.mean(mae))
    print("avg pear: ", np.mean(pear))
    print("std mse: ", np.std(mse))
    print("std mae: ", np.std(mae))
    print("std pear: ", np.std(pear))
    return mae, mse, pear, np.concatenate(predictions)

# ********** MAIN PROGRAM ********** #
X, y, X_TEST = load_data('train_t0.csv'), load_data('train_t1.csv'), load_data('test_t0.csv')
X, y, X_TEST = preprocessing(X, y, X_TEST)

ridge = train_model(X)
predictions = predict(X_TEST, ridge)
write_output("team14_final", predictions)

print("---5 CV---")
X, y, X_TEST = load_data('train_t0.csv'), load_data('train_t1.csv'), load_data('test_t0.csv')
mae, mse, pear, preds = cv5(X, y)
