"""
Target Problem:
---------------
* To train a model to predict the brain connectivity for the next time point given the brain connectivity at current time point.
Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* Preprocessing Method (if any) -> Dimensionality Reduction method (if any) -> Learner
Input to Proposed Solution:
---------------------------
* Directories of training and testing data in csv file format
* These two types of data should be stored in n x m pattern in csv file format.
  Typical Example:
  ----------------
  n x m samples in training csv file (Explain n and m) 
  k x s samples in testing csv file (Explain k and s

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "results_team11.csv" file. (Change the name file if needed)
Code Owner:
-----------
* Copyright © Team 11. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Spring/Fall 2020. All rights reserved. 
"""

import pandas as pd
import numpy as np
import math 
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import csv
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy.stats.stats import pearsonr
import random as r
r.seed(1)
np.random.seed(1)
import warnings
warnings.filterwarnings('ignore')

def load_data(csv):

    """
    The method reads train and test data from their dataset files.
    Then, it splits train data into features and labels.
    Parameters
    ----------
    train_file: directory of the file in which train data set is located
    test_file: directory of the file in which test data set is located
    """
    # reading the data from the csv files
    df = pd.read_csv(csv)
    # ignoring the index column of the data (0,...,149 or 0,...,79)
    df = df.drop(df.columns[0],axis=1)
    return df


def preprocessing(dataset, low, high):
 
    """
    * IQR(Interquartile range) function to find if the data is classified as outlier or not
    * returns a boolean array (if the corresponding data(row) is outlier it will be true otherwise false)
    ----------
    dataset: input data to process
    low: 
    high:
    """
    Q1 = dataset.quantile(low)
    Q3 = dataset.quantile(high)
    IQR = Q3 - Q1
    dataset_outlier_status = ((dataset < (Q1 - 1.5 * IQR)) |(dataset > (Q3 + 1.5 * IQR)))
    dataset_outlier_status = dataset_outlier_status.to_numpy()
    return dataset_outlier_status


def gradient_descent(x, y, x_outlier_status, y_outlier_status, feature_number, index, alpha):

    """
    This method is the implementation of the gradient descent
    Parameters
    ----------
    x: features of training data
    y: labels of training data
    x_outlier_status: outlier mask
    y_outlier_status: outlier mask
    feature_number: number of features
    index: required for plots
    alpha: learning rate
    """
    w = np.full((feature_number+1,1), 0.5) # holds bias in the first column
    counter = 0
    
    # the for loop below is to get rid of outliers according to our examination of the both training and testing datasets
    x_wo_outliers = np.empty([0, 1])
    y_wo_outliers = np.empty([0, 1])
    for i in range(0,x.size):
        if  False == x_outlier_status[i]:
            if False == y_outlier_status[i]:
                x_wo_outliers = np.insert(x_wo_outliers, counter, x[i], axis=0)
                y_wo_outliers = np.insert(y_wo_outliers, counter, y[i], axis=0)
                counter += 1
    
    # xtilda = [1 x]T
    x_wo_outliers = np.insert(x_wo_outliers, 0, 1, axis=1)
    
    number_of_iterations = 5000
    
    # iterative step for gradient descent
    for i in range (0,number_of_iterations):
        grad = 2 * np.dot(np.dot(x_wo_outliers.T,x_wo_outliers),w) - 2 * np.dot(x_wo_outliers.T,y_wo_outliers)
        grad = grad / (x_wo_outliers.shape[0])
        w = w - alpha * grad
    
    # if you want to plot the outlier status you can uncomment the below code
    # green means data is used in the training
    # red means data is ignored
    """
    x_wo_outliers = np.delete(x_wo_outliers, 0, 1)
    plot_outlier_status(x,y, x_wo_outliers, y_wo_outliers, index)
    
    """
    return w


def train_model(train_t0, train_t1):

    """
    The method creates a learning model and trains it by using training data.
    Parameters
    ----------
    train_t0: x
    train_t1: y
    """
    # if you run this you will get kaggle output
    # don't run it if you don't need kaggle  output,because it will take sometime to run
    ws = []
    for i in range(1,596):
        train_t0_single_feature = train_t0.iloc[:, [i-1]]
        train_t1_single_feature = train_t1.iloc[:, [i-1]]
        
        train_t0_outlier_status = preprocessing(train_t0_single_feature, 0.15, 0.85)
        train_t1_outlier_status = preprocessing(train_t1_single_feature, 0.15, 0.85)

        train_t0_single_feature = train_t0_single_feature.to_numpy()
        train_t1_single_feature = train_t1_single_feature.to_numpy()
        w = gradient_descent(train_t0_single_feature, train_t1_single_feature, train_t0_outlier_status, train_t1_outlier_status, 1, i, 0.9)
        ws.append(w)
    return ws

def predict(test_t0, ws):

    """
    The method predicts for testing data samples by using trained learning model.
    Parameters
    ----------
    x_tst: features of testing data
    model: trained learning model
    """
    predicted_data = pd.DataFrame()
    for i in range(1,596):
        test_t0_single_feature = test_t0.iloc[:,[i-1]]
        test_t0_single_feature = test_t0_single_feature.to_numpy()
        y_predicted = predict_single(test_t0_single_feature, ws[i-1])
        predicted_data.insert(i-1, "f" + str(i), y_predicted.reshape(80))
    return predicted_data

def predict_single(x, w):

    """
    The method predicts output for one feature
    Parameters
    ----------
    x_tst: features of testing data
    model: trained learning model
    """
    x = np.insert(x, 0, 1, axis=1)
    return np.dot(x,w)

def plot_outlier_status(x, y, x_wo_outlier, y_wo_outlier, index):
 
    """
    Plotting function for visualizing the data and outlier distribution
    Parameters
    ----------
    x: features of training data
    y: labels of training data
    x_wo_outlier: x without outliers
    y_wo_outlier: y without outliers
    index: required for plots
    """
    plt.scatter(x,y, color = "red")
    plt.scatter(x_wo_outlier,y_wo_outlier, color = "green")
    plt.title("f" + str(index))
    plt.xlabel("T0 Connectivity")
    plt.ylabel("T1 Connectivity")
    plt.show()


def write_output(filename, predictions):

    """
    Writes predictions as desired in the submission process
    Parameters
    ----------
    filename: file path for saving file
    predictions: model outputs
    """
    meltedDF = predictions.to_numpy().flatten()
    with open(filename+'.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["ID", "Predicted"])
        for i in range(0,47600):
            if(meltedDF[i] < 10 ** (-50)):
                meltedDF[i] = 0
            writer.writerow([i, meltedDF[i]])

def cv5(x, y):

    """
    Applies 5 fold cross validation on given dataset
    Parameters
    ----------
    x: train_t0
    y: train_t1
    """
    fold_number = 5
    kf = KFold(n_splits=fold_number, shuffle = True ,random_state=1)
    kf.get_n_splits(train_t0)

    y_pred = np.zeros([150, 595])
    mses = []
    maes = []
    pears = []

    for train_index, test_index in kf.split(train_t0):
        X_train, X_test = train_t0[train_index], train_t0[test_index]
        y_train, y_test = train_t1[train_index], train_t1[test_index]
        y_predicted_list = []
        
        test_arr = []
        preds_arr = []
        for i in range(1,596):
            X_train_single_feature = X_train[:,[i-1]]
            y_train_single_feature = y_train[:,[i-1]]
            
            X_test_single_feature = X_test[:,[i-1]]
            y_test_single_feature = y_test[:,[i-1]]
            
            X_train_single_feature = pd.DataFrame(data=X_train_single_feature)
            y_train_single_feature = pd.DataFrame(data=y_train_single_feature)
            
            X_train_outlier_status = preprocessing(X_train_single_feature, 0.15, 0.85)
            y_train_outlier_status = preprocessing(y_train_single_feature, 0.15, 0.85)
            
            X_train_single_feature = X_train_single_feature.to_numpy()
            y_train_single_feature = y_train_single_feature.to_numpy()
            
            w = gradient_descent(X_train_single_feature,y_train_single_feature, X_train_outlier_status, y_train_outlier_status, 1, i, 0.1)

            y_predicted = predict_single(X_test_single_feature,w)
            #we are collecting all the predicted t1 values into 150*595 matrix in this for loop
            z = 0
            for j in test_index:
                y_pred[j][i-1] = y_predicted[z]
                z += 1
            for i in range(30):
                y_predicted_list.append(y_predicted[i])

            test_arr.append(y_test_single_feature)
            preds_arr.append(y_predicted)

        test_arr = np.asarray(test_arr)
        test_arr = test_arr.reshape(595,30)
        preds_arr = np.asarray(preds_arr)
        preds_arr = preds_arr.reshape(595,30)
        mses.append(mean_squared_error(test_arr, preds_arr))
        maes.append(mean_absolute_error(test_arr, preds_arr))
        pears.append(pearsonr(test_arr.flatten(), preds_arr.flatten())[0])


    print("MSE: ", mses)
    print("MAE: ", maes)
    print("PCC: ", pears)

    print("Average error of five fold cross validation MSE:", np.sum(mses) / 5)
    print("Average error of five fold cross validation MAE:", np.sum(maes) / 5)
    print("Average error of five fold cross validation pearson:", np.sum(pears) / 5)

    print(" std of five fold cross validation MSE:", np.std(mses))
    print(" std of five fold cross validation MAE:", np.std(maes))
    print(" std of five fold cross validation pearson:", np.std(pears))

    predicted = y_pred.flatten()    
    #below code is for extracting the data as csv file for kfold
    with open('predictions.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["ID", "Predicted"])
        for i in range(0,89250):
            if(predicted[i] < 10 ** (-50)):
                predicted[i] = 0
            writer.writerow([i, predicted[i]])

# ********** MAIN PROGRAM ********** #
train_t0 = load_data("train_t0.csv")
train_t1 = load_data("train_t1.csv")
test_t0 = load_data("test_t0.csv")

weights = train_model(train_t0, train_t1)
predictions = predict(test_t0, weights)
write_output("results_team1", predictions)

train_t0 = train_t0.to_numpy()
train_t1 = train_t1.to_numpy()
cv5(train_t0, train_t1)

