"""
Target Problem:
---------------
* Predict the evolution of brain connectivity over time.

Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* Outlier Elimination with Isolation Forest -> Standard Scaler -> PCA -> RandomForest

Input to Proposed Solution:
---------------------------
* Directories of training data and labels(measurements at two different timepoints), and testing data in csv file format.
* These data should be stored in n x m pattern in csv file format.

  Typical Example:
  ----------------
  n x m samples in train_t0 csv file (n number of samples, m number of features)
  n x m samples in train_t1 csv file (n number of samples, m number of features)
  k x m samples in test_t0 csv file (k number of samples, m number of features)

* These data set files are ready by load_data() function.

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "submission.csv" file.

Code Owner:
-----------
* Copyright © Team 10. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Fall 2021. All rights reserved.
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest, RandomForestRegressor
from sklearn.preprocessing import StandardScaler, normalize, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error as mae
from sklearn.metrics import mean_squared_error as mse
from scipy.stats import pearsonr as pr
from scipy import stats
import statistics
import random
import os

np.random.seed(1)
random.seed(1)

def load_data(files):
    
    """
    The method reads train and test data from their dataset files.
    Then, it splits train data into features and labels.
    Parameters
    ----------
    train_file: directory of the file in which train data set is located
    test_file: directory of the file in which test data set is located
    """

    train_data = pd.read_csv(files[0]).drop("ID", axis = 1)
    train_label = pd.read_csv(files[1]).drop("ID", axis = 1)
    test_data = pd.read_csv(files[2]).drop("ID", axis = 1)
    return train_data, train_label, test_data

def preprocessing(x_tra, y_tra, x_tst):

    """
    * The method trains an Isolation Forest to detect and remove outliers in data.
    * Then, it standardizes the data.
    * Finally, it performs PCA on the data to reduce dimensionality.
    Parameters
    ----------
    x_tra: features of training data
    y_tra: labels of training data
    """
    diff = y_tra - x_tra
    clf = IsolationForest(random_state=0).fit(diff)
    k1 = clf.predict(diff)
    clf2 = IsolationForest(random_state=0).fit(x_tra)
    k2 = clf2.predict(x_tra)
    clf3 = IsolationForest(random_state=0).fit(y_tra)
    k3 = clf3.predict(y_tra)
    anomalies = np.logical_and(k3==1, np.logical_and(k2==1, k1==1))
    locations = np.where(anomalies)
    x_tra = x_tra.iloc[locations]
    y_tra = y_tra.iloc[locations]

    scaler = StandardScaler()
    scaler.fit(x_tra)
    x_tra = scaler.transform(x_tra)
    x_tst = scaler.transform(x_tst)

    pca = PCA(0.70)
    pca.fit(x_tra)
    x_tra = pca.transform(x_tra)
    x_tst = pca.transform(x_tst)
    return x_tra, y_tra, x_tst

def train(x_tra, y_tra):
    
    """
    The method creates a learning model and trains it by using training data.
    Parameters
    ----------
    x_tra: features of training data
    y_tra: labels of training data
    x_tst: features of testing data
    """

    model = RandomForestRegressor(n_estimators=200)

    return model.fit(x_tra, y_tra)

def predict(x_tst, model):
    
    """
    The method predicts labels for testing data samples by using trained learning model.
    Parameters
    ----------
    x_tst: features of testing data
    model: trained learning model
    """

    return model.predict(x_tst)


def cv(x_tra, y_tra):
    """
    Performs 5-fold cross validation.
    Parameters
    ----------
    x_tra: features of testing data
    y_tra: labels of testing data
    """

    kf = KFold(n_splits=5, shuffle=True, random_state=1)
    x_tra = x_tra.to_numpy()
    y_tra = y_tra.to_numpy()
    results = {'YourModel': []}
    msqr = []
    mabs = []
    pearson = []
    model = RandomForestRegressor(n_estimators=200)
    for train_index, test_index in kf.split(x_tra):
        train = x_tra[train_index, :]
        train_labels = y_tra[train_index, :]
        test = x_tra[test_index, :]
        test_labels = y_tra[test_index, :]

        diff = train_labels - train

        z = np.abs(stats.zscore(diff, 1))
        thresh = 4
        train = train[(z<thresh).all(axis=1)]
        train_labels= train_labels[(z<thresh).all(axis=1)]

        scaler = MinMaxScaler()
        scaler.fit(train)
        train = scaler.transform(train)
        test = scaler.transform(test)

        pca = PCA(0.70)
        pca.fit(train)
        train = pca.transform(train)
        test = pca.transform(test)
        model.fit(train, train_labels)
        test_predict = model.predict(test)
        actual = test_labels.flatten()
        predicted = test_predict.flatten()
        scores = mse(predicted, actual)
        results['YourModel'].append(scores)
        msqr.append(mse(predicted, actual))
        mabs.append(mae(predicted, actual))
        pearson.append(pr(predicted, actual))

    results_df = pd.DataFrame(results).T
    variance = results_df.var(axis=1)
    mean = results_df.mean(axis=1)
    results_df.insert(5, "Var", variance)
    results_df.insert(6, "Mean", mean)
    try:
        print(statistics.mean(msqr), statistics.stdev(msqr))
        print(statistics.mean(mabs), statistics.stdev(mabs))
        print(statistics.mean(i[0] for i in pearson), statistics.stdev(i[0] for i in pearson))
    except:
        pass

    return results_df

def write_output(filename, predictions):

    """
    Writes the outputted predictions to a file.
    Parameters
    ----------
    filename: file path for saving file
    predictions: numpy array object containing predictions
    """

    assert type(predictions) == np.ndarray, "Pass a numpy.ndarray"
    assert predictions.shape[0] == 80, f"Missing data points Expected 80 Got {predictions.shape[0]}"
    assert predictions.shape[1] == 595, f"Number of features predicted is not correct Excpected 595 Got {predictions.shape[1]}"
    assert type(filename) == str, "Filename must be a string"
    assert '.csv' in filename, "Add extension '.csv'"

    df = pd.DataFrame(predictions.flatten())
    df.index.name = "ID"
    df = df.rename(columns={0:"predicted"})
    df.to_csv(filename)
    print(f"{filename} saved at {os.getcwd()}")
    return df

filenames = ["train_t0.csv", "train_t1.csv", "test_t0.csv"]
x_tra, y_tra, x_tst = load_data(filenames)
cv_results = cv(x_tra, y_tra)
x_tra, y_tra, x_tst = preprocessing(x_tra, y_tra, x_tst)
model = train(x_tra, y_tra)
predictions = predict(x_tst, model)
write_output("submission.csv", predictions)