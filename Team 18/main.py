"""
Target Problem:
---------------
* Predict the evolution of brain connectivity over time.

Proposed Solution (Machine Learning Pipeline):
----------------------------------------------
* Adding Random Noise -> Extra Random Trees each trained on a single feature

Input to Proposed Solution:
---------------------------
* Directories of training data and labels(measurements at two different timepoints), and testing data in csv file format.
* These data should be stored in n x m pattern in csv file format.

  Typical Example:
  ----------------
  n x m samples in train_t0 csv file (n number of samples, m number of features)
  n x m samples in train_t1 csv file (n number of samples, m number of features)
  k x m samples in test_t0 csv file (k number of samples, m number of features)

* These data set files are ready by load_data() function.

Output of Proposed Solution:
----------------------------
* Predictions generated by learning model for testing set
* They are stored in "submission.csv" file.

Code Owner:
-----------
* Copyright © Team 18. All rights reserved.
* Copyright © Istanbul Technical University, Learning From Data Fall 2021. All rights reserved.
"""


import numpy as np
from sklearn.base import BaseEstimator
import pandas as pd
import random
from sklearn.model_selection import KFold
from sklearn.multioutput import  MultiOutputRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error as mae
from scipy.stats import pearsonr
import csv
from sklearn.model_selection import GridSearchCV

np.random.seed(1)
random.seed(1)

params = {  "bootstrap":True,
            "ccp_alpha":0.0,
            "criterion":'mse',
            "max_depth":5,
            "max_features":'auto',
            "max_leaf_nodes":None,
            "max_samples":None,
            "min_impurity_decrease":0.0,
            "min_impurity_split": None,
            "min_samples_leaf": 1,
            "min_samples_split": 15,
            "min_weight_fraction_leaf": 0.0,
            "n_estimators": 60,
            "n_jobs": None,
            "oob_score": False,
            "random_state": 1,
            "verbose":0,
            "warm_start":False}

def load_data(train_file, test_file):

    """
    The method reads train and test data from their dataset files.
    Then, it splits train data into features and labels.
    Parameters
    ----------
    train_file: directory of the file in which train data set is located
    test_file: directory of the file in which test data set is located
    """

    x_tra = pd.read_csv(train_file[0]).drop(columns=["ID"])
    y_tra = pd.read_csv(train_file[1]).drop(columns=["ID"])
    x_tst = pd.read_csv(test_file).drop(columns=["ID"])
    return x_tra, y_tra, x_tst


def preprocessing(x_tra, y_tra):

    """
    This method performs inverse sigmoid function on the data.
    ----------
    x_tra: features of training data
    y_tra: labels of training data
    """

    x_tra = np.array(x_tra)
    y_tra = np.array(y_tra)
    
    number_of_samples = x_tra.shape[0]
    min_random_cutoff = 100
    max_random_cutoff = 150
    noise = 0.1
    
    sample_indices = random.choices(range(x_tra.shape[0]), k = number_of_samples)
    
    cut_off_indices = []
    for sample_in in range(number_of_samples):
        number_of_drops = random.randrange(min_random_cutoff, max_random_cutoff)
        cut_off_indices.append(random.sample(range(x_tra.shape[1]), k = number_of_drops))
        
    X0_new = []
    X1_new = []    
    for i in sample_indices:
        sample = x_tra[i,:]
        if noise == 0:
            sample[cut_off_indices[i]] = 0
        else:
           sample[cut_off_indices[i]] += np.random.normal(loc = 0, scale = noise, size = (len(cut_off_indices[i])))
        X0_new.append(sample)
        target = y_tra[i,:]
        X1_new.append(target)
        
    return np.vstack((np.array(X0_new), x_tra)) , np.vstack((np.array(X1_new), y_tra))    

class Model(BaseEstimator):
    def __init__(self, **params):
        #Core estimator
        self.estimator =  MultiOutputRegressor(ExtraTreesRegressor(**params), n_jobs = -1)
        self.is_fitted_ = False

    #Augment data while fitting
    def fit(self, X, y):
        X, y = preprocessing(X, y)
        self.estimator.fit(X, y)
        self.is_fitted_ = True
        return self

    #No augmentation on the prediction
    def predict(self, X):
        preds = self.estimator.predict(X)
        return preds
    
    #Needed for the grid search, part of SKlearn custom estimator api
    def get_params(self, deep=True):
        return self.estimator.get_params(deep)
    
    #Needed for the grid search, part of SKlearn custom estimator api
    def set_params(self, **parameters):
        self.estimator.set_params(**parameters)
        return self
    
    #MSE score, grid search uses this by default 
    def score(self, X, y):
        return mean_squared_error(self.predict(X), y)
   

def train_model(x_tra, y_tra):

    """
    The method creates a learning model and trains it by using training data.
    Parameters
    ----------
    x_tra: features of training data
    y_tra: labels of training data
    """

    estimator = Model(**params)
    return estimator.fit(x_tra, y_tra)

def predict(x_tst, model):

    """
    The method predicts labels for testing data samples by using trained learning model.
    Parameters
    ----------
    x_tst: features of testing data
    model: trained learning model
    """

    return model.predict(x_tst)

def cross_validation(x_tra, y_tra):

    """
    The method performs 5-fold cross-validation on training data and returns the error rates.
    Parameters
    ----------
    x_tra: features of training data
    y_tra: labels of training data
    """

    models = []
    #For 5-fold CV
    kf = KFold(n_splits=5, shuffle=True, random_state=1)
    kf.get_n_splits(x_tra)
    scores = []
    maes = []
    pcc = []
    print("**************")
    x_tra = np.array(x_tra)
    y_tra = np.array(y_tra)
    for train_index, test_index in kf.split(x_tra):
        X_train, X_test = x_tra[train_index], x_tra[test_index]
        y_train, y_test = y_tra[train_index], y_tra[test_index]
        estimator = Model(**params)
        estimator.fit(X_train, y_train)
        models.append(estimator)
        preds = estimator.predict(X_test)
        score = mean_squared_error(preds, y_test)
        scores.append(score)
        maes.append(mae(preds, y_test))
        pcc.append(pearsonr(preds.flatten(), y_test.flatten())[0])
        print("Testing score for fold {}: {:.5f}".format(len(scores), score))
        print("Mean testing score across folds: {:.5f}".format(sum(scores) / len(scores)))

    print("Mean testing score across folds: {:.5f}".format(sum(scores) / len(scores)))
    print(maes)
    print(sum(maes)/5)
    print(pcc)
    print(sum(pcc)/5)

def write_output(filename, predictions):
    
    """
    Writes the outputted predictions to a file.
    Parameters
    ----------
    filename: file path for saving file
    predictions: pandas.dataframe object containing predictions
    """

    meltedDF = predictions.flatten()
    kaglexd = pd.DataFrame(meltedDF)
    kaglexd_melted = kaglexd.melt()
    kaglexd_melted.drop('variable', inplace=True, axis=1)

    kaglexd_melted = kaglexd_melted.rename(columns={'value': 'predicted'})
    kaglexd_melted.index.name = 'ID'

    kaglexd_melted.to_csv(filename)

# ********** MAIN PROGRAM ********** #


train_file, test_file = ["train_t0.csv", "train_t1.csv"], "test_t0.csv"
x_tra, y_tra, x_tst = load_data(train_file, test_file)

cross_validation(x_tra, y_tra)

model = train_model(x_tra, y_tra)
predictions = predict(x_tst, model)
write_output("submission.csv", predictions)